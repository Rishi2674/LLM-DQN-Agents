import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from utils.extract_local_context import extract_local_context, represent_local_context  # Import functions from utils
import numpy as np
from dotenv import load_dotenv
import os

load_dotenv()

ACCESS_TOKEN = os.getenv('ACCESS_TOKEN')

class LLMExperienceGenerator:
    def __init__(self, model_name="meta-llama/Llama-2-7b-hf", device='cpu'):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, token=ACCESS_TOKEN, legacy = True)
        self.llm = AutoModelForCausalLM.from_pretrained(model_name, token=ACCESS_TOKEN)
        self.device = device

    def generate_experience(self, prompt):
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        outputs = self.llm.generate(**inputs, do_sample=True, max_length=150) # Reduce max_length since its generating less data now
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response

    def extract_context_value(response):
        """
        Extracts the context value from the LLM response.

        Args:
            response (str): The response from the LLM.

        Returns:
            float: The context value as a floating-point number.
                Returns None if parsing fails.
        """
        try:
            # Split the response by ":" and extract the value
            context_value_str = response.split("Context Value :")[1].strip()
            
            # Convert the extracted value to a float
            context_value = float(context_value_str)
            
            return context_value

        except (IndexError, ValueError, AttributeError) as e:
            print(f"Error extracting context value: {e}, Response was: {response}")
            return None

    def create_prompt(self, state, action, reward, next_state, done, maze):
        """
        Creates a prompt for the LLM focusing on generating an informed context value.

        Args:
            state (str): Current state representation.
            action (int): Action taken.
            reward (float): Reward received.
            next_state (str): Next state representation.
            done (bool): Whether the episode is done.
            maze (np.array): The maze environment.

        Returns:
            str: A prompt to send to the LLM.
        """
        agent_position = self.decode_state(state)
        local_context = extract_local_context(maze, agent_position)
        local_context_str = represent_local_context(local_context)

        prompt = f"Local Context:\n{local_context_str}\n"
        prompt += f"State: {state}\n"
        prompt += f"Action: {action}\n"
        prompt += f"Reward: {reward}\n"
        prompt += f"Next State: {next_state}\n"
        prompt += f"Done: {done}\n"
        prompt += "Based on this information, provide a context value that can be used to improve the Q-value estimation."
        prompt += "The context value should help refine the action selection process."  # Added context about the role of the context value
        prompt += " The valid value of the context value should only be between -1 and 1"
        prompt += " Ensure that you the context value generated by the LLM takes into consideration of the local and global maze context"
        prompt += "Format: Context Value: [value]"

        return prompt


    def decode_state(self, state_string):
        try:
            row_str, col_str = state_string[1:-1].split(',')
            row = int(row_str.strip())
            col = int(col_str.strip())
            return (row, col)
        except ValueError:
            print(f"Invalid state string format: {state_string}")
            return None
